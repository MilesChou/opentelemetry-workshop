# See https://www.otelbin.io/
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

processors:
  batch:
  memory_limiter:
    check_interval: 5s
    limit_mib: 256
    spike_limit_mib: 128

  filter/application_health:
    traces:
      span:
        - 'attributes["http.route"] == "api/health" and resource.attributes["service.name"] == "web"'

  # See https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/tailsamplingprocessor/README.md
  tail_sampling:
    policies:
      - name: always-when-above-1ms
        type: latency
        latency:
          threshold_ms: 1
      - name: always-when-error
        type: status_code
        status_code:
          status_codes:
            - ERROR
      - name: sample-rate
        type: probabilistic
        probabilistic:
          sampling_percentage: 100
    decision_wait: 30s
    num_traces: 50000
    expected_new_traces_per_sec: 1000
    decision_cache:
      sampled_cache_size: 100000

exporters:
  debug:
    verbosity: detailed
  otlp/tempo:
    endpoint: "tempo:4317"
    tls:
      insecure: true
  prometheus:
    endpoint: "collector:8889"
    resource_to_telemetry_conversion:
      enabled: true
    enable_open_metrics: true
  loki:
    endpoint: "http://loki:3100/loki/api/v1/push"
    default_labels_enabled:
      exporter: false
      level: true

service:
  pipelines:
    traces:
      receivers:
        - otlp
      processors:
        #- memory_limiter
        - filter/application_health
        - tail_sampling
        #- batch
      exporters:
        - otlp/tempo
        - debug
    metrics:
      receivers:
        - otlp
      processors:
        #- memory_limiter
        #- batch
      exporters:
        - prometheus
        - debug
    logs:
      receivers:
        - otlp
      processors:
        #- memory_limiter
        #- batch
      exporters:
        - loki
        - debug

  telemetry:
    metrics:
      level: detailed
      address: 0.0.0.0:8888
